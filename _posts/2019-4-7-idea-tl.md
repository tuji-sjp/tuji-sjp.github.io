---
layout:     post
title:      "Search_度量方法调研与迁移学习"
subtitle:   " \"经过思路的整理，对用什么方法进行实验做了调研\""
date:       2019-04-7 13:00:00
author:     "jack"
header-img: "img/post-bg-sea2.jpg"
catalog: true
tags:
    - 可解释性
    - 迁移学习
    - 深度学习
---

## 调研与迁移学习

## 度量方法调研与迁移学习

### 1.理解迁移学习

重新理解我们的问题，我们目标也就是两个：在输出的feature map上度量时

1. 同类之间的差距小，不同类之间的差距大
2. 概率（准确率）高

在我们确定要去度量差距之后，我的重点就放在了度量方法的研究上。而刚好迁移学习的核心思想就是在做**源域到目标域的迁移**。通过度量两者之间的差距来调整或者`finetune`模型(上节组会还不知道这个名词，没想到在这遇见了)，而且我认为这些差距的度量其实就是在搞**分布概率和数值差距**来得到一个最优化目标。

跟传统的迁移学习相比，我们更关注**数值差距问题**。概率是结果

### 2. 度量方法调研

#### 2.1 基本距离方法

1. 欧式距离，衡量两个向量（空间上的点的距离）

   由于feature map数值上太不靠谱，我们要用的话，可以进行GAP之后压缩的1维的**标准化**欧式距离。

   ![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t2j2d6yhj317i05omx6.jpg)

2. 马氏距离，有点像标准欧式距离，先将两个点投射到一个分布中，再加协方差，**考虑方差**

   ![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wjqvdvxxj318x03wdfz.jpg)

   Σ 是这个分布的协方差。

   最典型的就是根据距离作判别问题，即假设有n个总体，计算某个样品X归属于哪一类的问题。此时虽然样品X离某个总体的欧氏距离最近，但是未必归属它，比如该总体的方差很小，说明需要非常近才能归为该类。对于这种情况，马氏距离比欧氏距离更适合作判别。

3. 余弦相似度，衡量两个向量的相关性 (夹角的余弦)。

4. 互信息，定义在两个概率分布 X,Y 上，`x ∈ X,y ∈ Y `

   ![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wk1y7q0rj30v603hq33.jpg)

#### 2.2 流形学习

我们关注重点在数据上，那么这个feature map的多维数据怎么看出它的差距就是关键。

> **Grassmann** 流形 G(d) 可以通过将原始的 d 维子空间 (特征向量) 看作它基础的元素，从而可以帮助学习分类器。在 **Grassmann** 流形中，特征变换和分布适配通常都有着有效的数值形式，因此在迁移学习问题中可以被很高效地表示和求解 。因此，我感觉利用 **Grassmann** 流形空间中来进行处理也是可行的。

#### 2.3 Maximum Mean Discrepancy

MMD最先提出的时候用于双样本的检测（two-sample test）问题，用于判断两个分布p和q是否相同。主要用来度量两个不同但相关的分布的距离。

最常用的场景就是**domain adaption**中衡量源域和目标域的距离。举个例子：深度迁移学习中网络就是一个函数,经过这个f(x)函数后，源域和目标域之间的差距越小，这个网络越能适用于目标域。

两个分布的距离定义为：

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wjvuffwxj317c04vt94.jpg)

其中 H 表示这个距离是由$$ ϕ()  ​$$将数据映射到再生希尔伯特空间（RKHS）中进行度量的。

关于$$ ϕ()  $$函数，不同的分布可以取不同的值，但是大多数（DDC, DAN），都是用高斯核函数 $$ k(u,v)=e{\frac{-∣∣u−v∣∣^2} {σ}} $$ 来作为核函数。

因为$$ ϕ()  $$函数比较难定义，所以就之间展开来进行计算。

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wl41qqcjj31cb05egmb.jpg)

代码实践：

```python
#!/usr/bin/env python
# encoding: utf-8

import torch
from torch.autograd import Variable

def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
    n_samples = int(source.size()[0])+int(target.size()[0])
    total = torch.cat([source, target], dim=0)
    total0 = total.unsqueeze(0).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    total1 = total.unsqueeze(1).expand(int(total.size(0)), int(total.size(0)), int(total.size(1)))
    L2_distance = ((total0-total1)**2).sum(2)
    if fix_sigma:
        bandwidth = fix_sigma
    else:
        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)
    bandwidth /= kernel_mul ** (kernel_num // 2)
    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]
    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for bandwidth_temp in bandwidth_list]
    return sum(kernel_val)#/len(kernel_val)

def mmd(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):
    batch_size = int(source.size()[0])
    kernels = guassian_kernel(source, target,
                              kernel_mul=kernel_mul, kernel_num=kernel_num,                                         fix_sigma=fix_sigma)
    XX = kernels[:batch_size, :batch_size]
    YY = kernels[batch_size:, batch_size:]
    XY = kernels[:batch_size, batch_size:]
    YX = kernels[batch_size:, :batch_size]
    loss = torch.mean(XX + YY - XY -YX)
    return loss
```

随机random生成两个不同的分布，随机取**10\*500**对应值去计算一下，然后相同分布计算值为0.7， 不同分布计算值7.4。虽然对内部的信息还有又不知道的地方，但是已经可以直观看出MMD判断差距的能力。

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wlb03mzrj317t0zxdk1.jpg)



也可以用于度量两个分布的相似性：如果对于所有以分布生成的样本空间为输入的函数f，如果两个分布生成的足够多的样本在f上的对应的像的均值都相等，那么那么可以认为这两个分布是同一个分布。、

### 3. 深度学习与迁移学习

#### 3.1 DCC

先看这个图吧，这是在两个不同的训练集上面进行迁移学习。而且使用loss的方法对两个网络fc的output array 进行惩罚来学习，不就是我们的第二种3.2事中学习的思想吗？对两个数据来进行比较。他们是两类，我们是一组（多个）。

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t47ixpllj30ew0fwtap.jpg)

DDC 方法 (Deep Domain Confusion) 解决深度网络的自适应问题。DDC 固定了 AlexNet 的前 7 层，在第 8 层 (分类器前一 层) 上加入了自适应的度量。自适应度量方法采用了被广泛使用的 MMD 准则。DDC 方法的损失函数表示为：

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1wljw5i48j30qm03gglm.jpg)

#### 3.2 DAN

DAN 方法 (Deep Adaptation Networks) 对 DDC 方法进行了几个方面的扩展。

- 有别于 DDC 方法只加入一个自适应层，DAN 方法同时加入了三个自适应层 (分类器前三层)。
- DAN方法采用了表征能力更好的多核MMD度量(MK-MMD代替了 DDC 方法中的单一核 MMD。
- DAN 方法将多核 MMD 的参数学习融入到深度网络的训练中，不增加网络的额外训练时间。DAN 方法在多个任务上都取得了比 DDC 更好的分类效果。 

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t49eytfuj30j00f5jvt.jpg)

也使用了t-she降维和对比了DDC和DAN的差距

![](https://ws1.sinaimg.cn/large/007bgNxTly1g1t4ajafa1j30z60ajdjh.jpg)

大致看了看这两篇论文，后面找时间再拜读。

[Learning Transferable Features with Deep Adaptation Networks](https://arxiv.org/pdf/1502.02791)

[DeepDomainConfusion: MaximizingforDomainInvariance](https://arxiv.org/pdf/1412.3474.pdf)

### 4. 总结

- 事后解释性，引用迁移学习中的MMD方法来度量差距，可以先对一部分数据进行MMD差距看看结果。
- 针对上一点，有一个不同点就是我们类会有很多，不一定每一个都好
- 事中解释性，我们可以对input和output上加MMD loss来惩罚网络。优点：输入不变，可以比较好训练
- 或者是对两个不同类别的output来MMD loss来训练。优点：都在feature map上维度一致。

最后回扣到我们的最初的解释性，feature map resize到原图大小后，可以只对比location 位置的像素。这样不就是可以满足我们最初的目标：**在即分类和定位的网络上解释**